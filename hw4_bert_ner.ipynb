{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bpanny/nlp-hw4/blob/main/hw4_bert_ner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tune BERT-based models from Hugging Face on CoNLL-2002 Spanish NER data\n",
        "\n",
        "In this notebook, you will fine-tune and evaluate multiple BERT-based models on CoNLL-2002 Spanish NER data.\n",
        "\n",
        "Code for loading and preprocessing the data is provided. You will provide code for training and evaluation using Hugging Face Trainer or PyTorch.\n",
        "\n",
        "Please copy this notebook and name it `{pitt email id}_hw4_bert_ner.ipynb`.\n",
        "\n",
        "Run all the cells starting from the top, filling in any sections that need to be filled in. Spots you need to fill in are specified.\n",
        "\n",
        "**Note**: Please run on GPU by going to Runtime > Change Runtime Type > T4 GPU\n",
        "\n",
        "This notebook is based on:\n",
        "* https://github.com/laxmimerit/NLP-Tutorials-with-HuggingFace/blob/main/NLP_with_HuggingFace_Tutorial_2_NER_Training.ipynb  \n",
        "* https://skimai.com/how-to-fine-tune-bert-for-named-entity-recognition-ner/"
      ],
      "metadata": {
        "id": "v-bsGmLEA1sN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up environment, preprocess data"
      ],
      "metadata": {
        "id": "zUZtkiLxA_Vv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aVyu5E0Anku"
      },
      "outputs": [],
      "source": [
        "# Download and install needed Hugging Face packages\n",
        "\n",
        "!pip install -U transformers\n",
        "!pip install -U accelerate\n",
        "!pip install -U datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset, which contains splits for training, validation (dev), and test\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset('conll2002', 'es')\n",
        "data"
      ],
      "metadata": {
        "id": "Q5zaRzS5A8Eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the tagset. Note the BIO framework with 4 possible types\n",
        "\n",
        "tags = data['train'].features['ner_tags'].feature\n",
        "\n",
        "index2tag = {idx:tag for idx, tag in enumerate(tags.names)}\n",
        "tag2index = {tag:idx for idx, tag in enumerate(tags.names)}\n",
        "index2tag"
      ],
      "metadata": {
        "id": "cU80eh79CE8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Put human-readable NER tags in data\n",
        "\n",
        "def create_tag_names(batch):\n",
        "  tag_name = {'ner_tags_str': [tags.int2str(idx) for idx in batch['ner_tags']]}\n",
        "  return tag_name\n",
        "\n",
        "data = data.map(create_tag_names)"
      ],
      "metadata": {
        "id": "6dMaXCoNCXIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the data\n",
        "pd.DataFrame(data['train'])[['tokens', 'ner_tags', 'ner_tags_str']].head(3)"
      ],
      "metadata": {
        "id": "ZJDn7lj7CgT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics\n",
        "Load NER-specific evaluation metrics"
      ],
      "metadata": {
        "id": "HrzZ7suyFiJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval\n",
        "!pip install evaluate\n",
        "\n",
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "metric = evaluate.load('seqeval')\n",
        "ner_feature = data['train'].features['ner_tags']\n",
        "label_names = ner_feature.feature.names\n",
        "labels = data['train'][0]['ner_tags']\n",
        "labels = [label_names[i] for i in labels]\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "  logits, labels = eval_preds\n",
        "\n",
        "  predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "  true_labels = [[label_names[l] for l in label if l!=-100] for label in labels]\n",
        "\n",
        "  true_predictions = [[label_names[p] for p,l in zip(prediction, label) if l!=-100]\n",
        "                      for prediction, label in zip(predictions, labels)]\n",
        "\n",
        "  all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "\n",
        "  return {\"precision\": all_metrics['overall_precision'],\n",
        "          \"recall\": all_metrics['overall_recall'],\n",
        "          \"f1\": all_metrics['overall_f1'],\n",
        "          \"accuracy\": all_metrics['overall_accuracy']}"
      ],
      "metadata": {
        "id": "I48I5wJJFui0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tune models\n",
        "This section is where you choose models and fill in parts of the code to do fine-tuning.\n",
        "\n",
        "You need to fine-tune at least 2 pretrained models from the Hugging Face platform on the preprocessed CoNLL-2002 Spanish data:\n",
        "* One BERT-based model pretrained with a regular masked language modeling (MLM) objective on a Spanish corpus. Examples: `PlanTL-GOB-ES/roberta-base-bne`, `chriskhanhtran/spanberta`\n",
        "* One model pretrained to perform NER on another language, such as English. Models pretrained on the CoNLL-2003 dataset often work. Examples: `elastic/distilbert-base-cased-finetuned-conll03-english`, `dbmdz/bert-bert-cased-finetuned-conll03-english`\n",
        "\n",
        "You'll want to make sure whatever pretrained model is cased, which contains valuable information for NER."
      ],
      "metadata": {
        "id": "PT9gTx24C44k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FILL IN which model you are fine-tuning and assign the name of it to the `pretrained_model` variable\n",
        "\n",
        "pretrained_model ="
      ],
      "metadata": {
        "id": "e7yVPOgkZ_9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the data with the pretrained model's tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model, use_fast=True, add_prefix_space=True)\n",
        "\n",
        "def align_labels_with_tokens(labels, word_ids):\n",
        "  new_labels = []\n",
        "  current_word=None\n",
        "  for word_id in word_ids:\n",
        "    if word_id != current_word:\n",
        "      current_word = word_id\n",
        "      label = -100 if word_id is None else labels[word_id]\n",
        "      new_labels.append(label)\n",
        "\n",
        "    elif word_id is None:\n",
        "      new_labels.append(-100)\n",
        "\n",
        "    else:\n",
        "      label = labels[word_id]\n",
        "\n",
        "      if label%2==1:\n",
        "        label = label + 1\n",
        "      new_labels.append(label)\n",
        "\n",
        "  return new_labels\n",
        "\n",
        "def tokenize_and_align_labels(examples):\n",
        "  tokenized_inputs = tokenizer(examples['tokens'], truncation=True, is_split_into_words=True)\n",
        "\n",
        "  all_labels = examples['ner_tags']\n",
        "\n",
        "  new_labels = []\n",
        "  for i, labels in enumerate(all_labels):\n",
        "    word_ids = tokenized_inputs.word_ids(i)\n",
        "    new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
        "\n",
        "  tokenized_inputs['labels'] = new_labels\n",
        "\n",
        "  return tokenized_inputs\n",
        "tokenized_datasets = data.map(tokenize_and_align_labels, batched=True, remove_columns=data['train'].column_names)\n",
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "Hr5GJU3TC3o-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a data collator to handle batching\n",
        "\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "RWb_R4pJFlMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train (fine-tune) the model"
      ],
      "metadata": {
        "id": "ey-86X_EGQCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {i:label for i, label in enumerate(label_names)}\n",
        "label2id = {label:i for i, label in enumerate(label_names)}\n",
        "print(id2label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDyIyfQtGSmL",
        "outputId": "e7651157-daa8-4c1b-bf83-e42b4ad79bc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC', 7: 'B-MISC', 8: 'I-MISC'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(pretrained_model,\n",
        "                                                    id2label=id2label,\n",
        "                                                    label2id=label2id)"
      ],
      "metadata": {
        "id": "xKTZcdnFGztB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FILL IN code to train\n",
        "Provide code to train (fine-tune) the pretrained model.\n",
        " You can use Hugging Face Trainer class or use any other package you want, such as PyTorch.\n",
        "\n",
        " See the [Hugging Face Trainer user guide](https://huggingface.co/learn/nlp-course/chapter3/3?fw=pt) or use any other online examples/resources you find online."
      ],
      "metadata": {
        "id": "WC_kv1_Kegue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training code here"
      ],
      "metadata": {
        "id": "F_ABrmw-H-X8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the fine-tuned model"
      ],
      "metadata": {
        "id": "D-6114reLoKZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FILL IN code to evaluate performance of the model on the test set\n",
        "Provide code to evaluate the pretrained model on the `test` portion of the dataset (`tokenized_datasets['test']`)\n",
        "\n",
        "You'll need the F1 score for your report.\n",
        "This is calculated automatically if you passed the `compute_metrics` function to the `Trainer` class."
      ],
      "metadata": {
        "id": "hpZE3kHhhXz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing code here"
      ],
      "metadata": {
        "id": "EWaWR04hLerC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hooray, you're done evaluating a model!\n",
        "\n",
        "Feel free to restart the runtime and evaluate another one, or test that model on an example in the section below (which you'll need to do for at least one model)."
      ],
      "metadata": {
        "id": "vQNbWyzjkQVZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the model on an example\n",
        "Code is provided here to test your fine-tuned classifier on an example sentence.\n",
        "\n",
        "You will need to fill in the path to a checkpoint of your fine-tuned model if it has been saved somewhere. Or feel free to run your model some other way on the example sentence.\n",
        "\n",
        "You will need the output of running at least one of your models on the example sentence for your report."
      ],
      "metadata": {
        "id": "UbcmLk0yiwOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test performance on an example\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "checkpoint = # FILL IN path to one of the checkpoints of your fine-tuned model\n",
        "token_classifier = pipeline(\n",
        "    \"token-classification\", model=checkpoint, aggregation_strategy=\"simple\"\n",
        ")\n",
        "\n",
        "test_sentence = \"Mi nombre is Miguel Salgado. Trabajo en la Universidad de Pittsburgh y vivo en Pittsburgh.\"\n",
        "token_classifier(test_sentence)"
      ],
      "metadata": {
        "id": "A9L6wbiBJMhI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}